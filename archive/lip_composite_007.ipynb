{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vmatic\\AppData\\Local\\Continuum\\Anaconda3_ticma\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:161: UserWarning: pylab import has clobbered these variables: ['f', 'correlate']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "%pylab\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202\n",
      "202\n"
     ]
    }
   ],
   "source": [
    "# Load .csv files\n",
    "# these files are created after the face has been detected\n",
    "# it stores lip positions for 2 (or more) speakers. \n",
    "\n",
    "\n",
    "id_file = 5\n",
    "a1 = read_csv('C:/Users/vmatic/Dropbox/Upwork/Database/database_002/lips'+ str(id_file) +'_1.csv', header =0)\n",
    "a2 = read_csv('C:/Users/vmatic/Dropbox/Upwork/Database/database_002/lips'+ str(id_file) +'_2.csv', header =0)\n",
    "print(a1.shape[0])\n",
    "print(a2.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202\n"
     ]
    }
   ],
   "source": [
    "frame_max = max( a1['Frame'].iloc[-1], a2['Frame'].iloc[-1])\n",
    "print(frame_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# it may happen that face detection algorithm fails to detect a face\n",
    "# for this reason we have developed an extrapolation of the 'lip signal'. \n",
    "# this is the only way to solve it since we cannot change face detection algorith code\n",
    "\n",
    "#X1 - are x postitions of lips for the first speaker\n",
    "#Y1 - are y postitoins of lips for the first speaker\n",
    "\n",
    "# w1 shows how large the \"head\" is, that is, we experimented how it affects the 'lip signal'\n",
    "# e.g. if one person is very close to the camera, the signals can be different as for someone who is rather far \n",
    "# (face will be smaller)\n",
    "\n",
    "X1 = np.zeros((frame_max,24))\n",
    "frame_ind = a1['Frame'].values\n",
    "frame_ind = frame_ind-1\n",
    "X1[frame_ind,:] = a1.iloc[:,1:25]\n",
    "\n",
    "Y1 = np.zeros((frame_max,24))\n",
    "frame_ind = a1['Frame'].values\n",
    "frame_ind = frame_ind-1\n",
    "Y1[frame_ind,:] = a1.iloc[:,25:49]\n",
    "\n",
    "X2 = np.zeros((frame_max,24))\n",
    "frame_ind = a2['Frame'].values\n",
    "frame_ind = frame_ind-1\n",
    "X2[frame_ind,:] = a2.iloc[:,1:25]\n",
    "\n",
    "Y2 = np.zeros((frame_max,24))\n",
    "frame_ind = a2['Frame'].values\n",
    "frame_ind = frame_ind-1\n",
    "Y2[frame_ind,:] = a2.iloc[:,25:49]\n",
    "\n",
    "W1 = np.zeros((frame_max))\n",
    "frame_ind = a1['Frame'].values\n",
    "frame_ind = frame_ind-1\n",
    "W1[frame_ind] = a1.iloc[:,-1]\n",
    "\n",
    "W2 = np.zeros((frame_max))\n",
    "frame_ind = a2['Frame'].values\n",
    "frame_ind = frame_ind-1\n",
    "W2[frame_ind] = a2.iloc[:,-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here we have optimized, what 'lip points' are optimal to be used for further processing\n",
    "\n",
    "index_dots = np.array([1,2,3,4,5,6,14,15,16,17,18])\n",
    "index_dots = index_dots - 1\n",
    "\n",
    "#X1 = a1.iloc[:,1:25]\n",
    "#X1 = X1.as_matrix()\n",
    "X1 = X1[:,index_dots]\n",
    "#Y1 = a1.iloc[:,25:49]\n",
    "#Y1 = Y1.as_matrix()\n",
    "Y1 = Y1[:, index_dots]\n",
    "\n",
    "#X2 = a2.iloc[:,1:25]\n",
    "#X2 = X2.as_matrix()\n",
    "X2 = X2[:,index_dots]\n",
    "#Y2 = a2.iloc[:,25:49]\n",
    "#Y2 = Y2.as_matrix()\n",
    "Y2 = Y2[:,index_dots]\n",
    "#W2 = a2.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here we perform an interpolation of the \"missed frames\" due to fails of the algorithm for face detection\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "for i in range((X1.shape[1])):\n",
    "    temp = X1[:,i]\n",
    "    ind_temp = np.arange(len(temp))\n",
    "    idx    = np.where(temp!=0)\n",
    "    f = interp1d(ind_temp[idx], temp[idx])\n",
    "    X1[:,i] = f(ind_temp)\n",
    "   \n",
    "    temp = Y1[:,i]\n",
    "    ind_temp = np.arange(len(temp))\n",
    "    idx    = np.where(temp!=0)\n",
    "    f = interp1d(ind_temp[idx], temp[idx])\n",
    "    Y1[:,i] = f(ind_temp)\n",
    "    \n",
    "    temp = X2[:,i]\n",
    "    ind_temp = np.arange(len(temp))\n",
    "    idx    = np.where(temp!=0)\n",
    "    f = interp1d(ind_temp[idx], temp[idx])\n",
    "    X2[:,i] = f(ind_temp)\n",
    "    \n",
    "    temp = Y2[:,i]\n",
    "    ind_temp = np.arange(len(temp))\n",
    "    idx    = np.where(temp!=0)\n",
    "    f = interp1d(ind_temp[idx], temp[idx])\n",
    "    Y2[:,i] = f(ind_temp)\n",
    "    \n",
    "temp = W1\n",
    "ind_temp = np.arange(len(temp))\n",
    "idx    = np.where(temp!=0)\n",
    "f = interp1d(ind_temp[idx], temp[idx])\n",
    "W1 = f(ind_temp)\n",
    "    \n",
    "temp = W2\n",
    "ind_temp = np.arange(len(temp))\n",
    "idx    = np.where(temp!=0)\n",
    "f = interp1d(ind_temp[idx], temp[idx])\n",
    "W2 = f(ind_temp)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyplot.plot(Y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We assume that lips are forming a convex polygon \n",
    "# to calculate area of the lips, we split it into triangles \n",
    "# we sum up all triangles and calculate area of the 'mouth'\n",
    "\n",
    "\n",
    "def area(X, Y):\n",
    "    n = len(X)\n",
    "    A = 0\n",
    "    for i in range(len(X)-2) :\n",
    "        temp = area_triangle( X[i], Y[i], X[i+1], Y[i+1], X[i+2], Y[i+2])\n",
    "        A = A + temp\n",
    "    \n",
    "    return A\n",
    "\n",
    "def area_triangle(x1, y1, x2, y2, x3, y3):\n",
    "    a = np.sqrt( (x1-x2)**2  +  (y1 - y2)**2 )\n",
    "    b = np.sqrt( (x1-x3)**2  +  (y1 - y3)**2 )\n",
    "    c = np.sqrt( (x3-x2)**2  +  (y3 - y2)**2 )\n",
    "    s = (a+b+c) / 2\n",
    "    area = np.sqrt (s * (s-a) * (s-b) * (s-c) )\n",
    "    \n",
    "    return area\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for each video frame we calculate mouth area \n",
    "# thus we obtain the mouth area signal for each frame\n",
    "\n",
    "def area_signal(X, Y):\n",
    "    signal1 = np.zeros(X.shape[0])\n",
    "    for i in range(X.shape[0]):\n",
    "        signal1[i] = area(X[i,:], Y[i,:])\n",
    "        \n",
    "    \n",
    "    return signal1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vmatic\\AppData\\Local\\Continuum\\Anaconda3_ticma\\lib\\site-packages\\ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in sqrt\n"
     ]
    }
   ],
   "source": [
    "#W1 = 1.0 / W1\n",
    "#W2 = 1.0 / W2\n",
    "\n",
    "# mouth area signals are created for a set of points - that is their, X and Y coordinates\n",
    "x_area1 = area_signal(X1, Y1) #* W1 * W1\n",
    "x_area2 = area_signal(X2, Y2) #* W2 * W2\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pyplot.plot(Y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we load autio file\n",
    "\n",
    "import scipy.io.wavfile \n",
    "Audio = scipy.io.wavfile.read('C:/Users/vmatic/Dropbox/Upwork/Database/database_002/lips'+ str(id_file)+ '_audio.wav')\n",
    "Audio = Audio[1][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# basic audio processing\n",
    "\n",
    "Audio = Audio - Audio.mean()\n",
    "Audio = Audio / Audio.std() \n",
    "Fs =  44100\n",
    "t = np.arange(0, len(Audio))\n",
    "t = t / Fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we calculate in this way the frame rate\n",
    "frame_rate = X1.shape[0] / Audio.shape[0] * 44100  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.662901824500434"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1162510"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Audio.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we calculate hilbert transform and calculate Audio envelope\n",
    "from scipy.signal import hilbert\n",
    "analytic_signal = hilbert(Audio)\n",
    "amplitude_envelope = np.abs(analytic_signal)\n",
    "instantaneous_phase = np.unwrap(np.angle(analytic_signal))\n",
    "fs = 44100\n",
    "instantaneous_frequency = (np.diff(instantaneous_phase) / (2.0*np.pi) * fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we resample audio signal to the frame rate\n",
    "# only in this way we are able to correlate the mouth area signal and 'audio envelope'\n",
    "from scipy import signal\n",
    "Fs = 44100\n",
    "x_audio = signal.resample(amplitude_envelope,  round( len(amplitude_envelope) / Fs  * frame_rate)   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11367e10>]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyplot.plot(x_area1, 'b')\n",
    "pyplot.plot(x_area2, 'k')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "x_area1 = np.nan_to_num(x_area1)\n",
    "x_area2 = np.nan_to_num(x_area2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we have processed all three signals -  bandpassed them to better capture the common dynamics\n",
    "\n",
    "x_area1 = x_area1 - x_area1[0]\n",
    "x_area2 = x_area2 - x_area2[0]\n",
    "b, a = sp.signal.butter(6, [0.1, 0.5], btype='bandpass')\n",
    "x1 = sp.signal.lfilter(b, a, x_area1)\n",
    "x2 = sp.signal.lfilter(b, a, x_area2)\n",
    "x0 = sp.signal.lfilter(b, a, x_audio)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "w, H = sp.signal.freqz(b, a)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pyplot.plot(abs(H))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyplot.plot(x_area2)\n",
    "pyplot.plot(x2,'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.signal import correlate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#here we compare 2 mouth area signals \n",
    "# x1 and x2 \n",
    "# with audio envelope x0\n",
    "\n",
    "# these signals segments are compared within the sliding window that slides through signals\n",
    "# window is the number of samples used for comparison\n",
    "# slide is a step that we use to move the signal\n",
    "# we store maximum of cross correlation function in C1 variable\n",
    "\n",
    "\n",
    "window = 20\n",
    "slide = 2\n",
    "duzina  = int(np.ceil  ((len(x0)-window) / slide))\n",
    "\n",
    "C1 = np.zeros( (duzina,2))\n",
    "for i in range (duzina):\n",
    "    index_begin = i*slide  \n",
    "    index_end   = (i+1) * slide + window\n",
    "    temp1= correlate(x0[index_begin:index_end] , x1[index_begin:index_end] )\n",
    "    temp11 = np.max(np.absolute(temp1))\n",
    "    C1[i,0]  = temp11\n",
    "    temp2= correlate(x0[index_begin:index_end] , x2[index_begin:index_end] )\n",
    "    temp21 = np.max(np.absolute(temp2))\n",
    "    C1[i,1]  = temp21\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 30)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_index = np.arange(0+int(window/2),int(window/2)+duzina*slide, slide) / frame_rate\n",
    "pyplot.subplot(311)\n",
    "pyplot.plot(np.arange(0,len(x0      )) / frame_rate , x0,'r')\n",
    "pyplot.ylabel('Audio envelope')\n",
    "\n",
    "pyplot.xlim( 0,30  )\n",
    "pyplot.subplot(312)\n",
    "\n",
    "pyplot.plot(np.arange(0,len(x0      )) / frame_rate , x1,'b')\n",
    "pyplot.plot(np.arange(0,len(x0      )) / frame_rate , x2,'k')\n",
    "pyplot.ylabel('Mouth area signals')\n",
    "\n",
    "pyplot.xlim(0, 30)\n",
    "pyplot.subplot(313)\n",
    "\n",
    "pyplot.plot(t_index, C1[:,0], 'b')\n",
    "pyplot.plot(t_index, C1[:,1], 'k')\n",
    "pyplot.xlabel('Two cross-correlation fs for every window: max value determines the speaker')\n",
    "\n",
    "pyplot.xlim(0, 30)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pyplot.plot(temp1,'b')\n",
    "pyplot.plot(temp2,'k')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
